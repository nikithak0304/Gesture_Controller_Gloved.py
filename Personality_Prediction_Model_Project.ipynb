{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikithak0304/Gesture_Controller_Gloved.py/blob/main/Personality_Prediction_Model_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adpLLSc4c1RS"
      },
      "source": [
        "### Downloading datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmJkOr4kdhmG"
      },
      "outputs": [],
      "source": [
        "pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbl_paClxi4Q"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "od.download('https://www.kaggle.com/datasnaek/mbti-type')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Uj0OnbdujA"
      },
      "outputs": [],
      "source": [
        "od.download('https://www.kaggle.com/kaggle/meta-kaggle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fanpPNiqfELN"
      },
      "source": [
        "### importing all required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA9a20-Iezzh"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from time import time\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvqO-EpFf90p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print('mbti-type : ',os.listdir('mbti-type'))\n",
        "print('meta-kaggle : ',os.listdir('meta-kaggle'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Y-7b0cgGOu"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('mbti-type/mbti_1.csv')\n",
        "forum_data = pd.read_csv('meta-kaggle/ForumMessages.csv')\n",
        "mbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition',\n",
        "        'S':'Sensing', 'T':'Thinking', 'F': 'Feeling',\n",
        "        'J':'Judging', 'P': 'Perceiving'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxpzxSzCNxy5"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWT2Jc7oh45W"
      },
      "source": [
        "### Let's view all datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9y8rndHgkFf"
      },
      "outputs": [],
      "source": [
        "print(train_data.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERfZKBZUgosD"
      },
      "outputs": [],
      "source": [
        "print(forum_data.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkyNNISiiDDm"
      },
      "source": [
        "### Let's view some info about our trainning dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6KG-9vkhZNP"
      },
      "outputs": [],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQb-MqfSiGtP"
      },
      "outputs": [],
      "source": [
        "type_count = train_data['type'].value_counts()\n",
        "colors = sns.color_palette(\"pastel\")\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(x=type_count.index, y=type_count.values, alpha=0.8, palette=colors)\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Types', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIexqz7Uih8y"
      },
      "source": [
        "### Handle missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHPCgJc0ii8C"
      },
      "outputs": [],
      "source": [
        "##ForumMessages.csv\n",
        "print('Forum Missing Values:')\n",
        "print(forum_data.isnull().sum())\n",
        "\n",
        "##mbti_1.csv\n",
        "print('Training Missing Values:')\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "forum_data['Message'] = forum_data['Message'].fillna('')\n",
        "\n",
        "print('Forum Missing Values:')\n",
        "print(forum_data.isnull().sum())\n",
        "\n",
        "print(forum_data['PostUserId'].value_counts())\n",
        "\n",
        "forum_data_agg = forum_data.groupby('PostUserId')['Message'].agg(lambda col: ' '.join(col)).reset_index()\n",
        "print(forum_data_agg['PostUserId'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIxPYVNkk_NK"
      },
      "source": [
        "### Cleaning data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pWELY19k-Va"
      },
      "outputs": [],
      "source": [
        "#function to clean data\n",
        "def clean_text(text):\n",
        "    #get rid of html and seperators\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = re.sub(r'\\|\\|\\|', r'  ', text)\n",
        "    text = re.sub(r'http\\S+', r'  ', text)\n",
        "    #get rid of punctuation\n",
        "    text = text.replace('.', '  ')\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    #get rid of numbers\n",
        "    text = ''.join(i for i in text if not i.isdigit())\n",
        "    return text\n",
        "\n",
        "train_data['clean_posts'] = train_data['posts'].apply(clean_text)\n",
        "train_data['clean_posts'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhXSUclflese"
      },
      "outputs": [],
      "source": [
        "forum_data_agg['clean_messages'] = forum_data_agg['Message'].apply(clean_text)\n",
        "forum_data_agg['clean_messages'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeaZfb-4lju2"
      },
      "outputs": [],
      "source": [
        "#function to split string by uppercase\n",
        "def split_uppercase(text):\n",
        "    text_list = text.split()\n",
        "    new_list = []\n",
        "    for i in text_list:\n",
        "        if i.isupper() == False: #don't split acronyms\n",
        "            word = re.sub(r'([A-Z])', r' \\1', i)\n",
        "            new_list.append(word)\n",
        "        else:\n",
        "            word = i\n",
        "            new_list.append(word)\n",
        "    words = ' '.join(new_list)\n",
        "    return words\n",
        "\n",
        "forum_data_agg['clean_messages'] = forum_data_agg['clean_messages'].apply(split_uppercase)\n",
        "forum_data_agg['clean_messages'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cTldeA0lvNo"
      },
      "outputs": [],
      "source": [
        "#function to stem words\n",
        "def stem_text(text):\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    words_list = text.split()\n",
        "    new_list = []\n",
        "    for i in words_list:\n",
        "        word = stemmer.stem(i)\n",
        "        new_list.append(word)\n",
        "\n",
        "    words = new_list\n",
        "    words = ' '.join(words)\n",
        "    return words\n",
        "train_data['clean_posts'] = train_data['clean_posts'].apply(stem_text)\n",
        "train_data['clean_posts'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECz5cjzcmALx"
      },
      "outputs": [],
      "source": [
        "forum_data_agg['clean_messages'] = forum_data_agg['clean_messages'].apply(stem_text)\n",
        "forum_data_agg['clean_messages'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TahRqesVnhFD"
      },
      "source": [
        "# **Classification(Classifier Model)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mWqiNrrnkoD"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F-e4N9Rnm8k"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "scoring = {'acc': 'accuracy',\n",
        "           'neg_log_loss': 'neg_log_loss',\n",
        "           'f1_micro': 'f1_micro'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjeZ8n8YsD0F"
      },
      "source": [
        "## ExtraTreesClassifier with SVD(single value decomposition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQSeiytUsL5G"
      },
      "outputs": [],
      "source": [
        "etc = ExtraTreesClassifier(n_estimators = 20, max_depth=4, n_jobs = -1)\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
        "tsvd = TruncatedSVD(n_components=10)\n",
        "model = Pipeline([('tfidf1', tfidf), ('tsvd1', tsvd), ('etc', etc)])\n",
        "\n",
        "\n",
        "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "results = cross_validate(model, train_data['clean_posts'], train_data['type'], cv=kfolds,\n",
        "                          scoring=scoring, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24HTJCmwsdlb"
      },
      "outputs": [],
      "source": [
        "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results['test_acc']),\n",
        "                                                  np.std(results['test_acc'])))\n",
        "\n",
        "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results['test_f1_micro']),\n",
        "                                            np.std(results['test_f1_micro'])))\n",
        "\n",
        "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results['test_neg_log_loss']),\n",
        "                                                 np.std(-1*results['test_neg_log_loss'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1yxCdMnsmtv"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GbGS0rWsnoa"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "tfidf2 = CountVectorizer(ngram_range=(1, 1),\n",
        "                         stop_words='english',\n",
        "                         lowercase = True,\n",
        "                         max_features = 5000)\n",
        "\n",
        "model_nb = Pipeline([('tfidf1', tfidf2), ('nb', MultinomialNB())])\n",
        "\n",
        "results_nb = cross_validate(model_nb, train_data['clean_posts'], train_data['type'], cv=kfolds,\n",
        "                          scoring=scoring, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSsy9w7uswS5"
      },
      "outputs": [],
      "source": [
        "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_acc']),\n",
        "                                                  np.std(results_nb['test_acc'])))\n",
        "\n",
        "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_f1_micro']),\n",
        "                                            np.std(results_nb['test_f1_micro'])))\n",
        "\n",
        "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_nb['test_neg_log_loss']),\n",
        "                                                 np.std(-1*results_nb['test_neg_log_loss'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS7MGd9_sykB"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km0Ie3wqszZS"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "tfidf2 = CountVectorizer(ngram_range=(1, 1), stop_words='english', lowercase = True, max_features = 5000)\n",
        "\n",
        "model_lr = Pipeline([('tfidf1', tfidf2), ('lr', LogisticRegression(class_weight=\"balanced\", C=0.005))])\n",
        "\n",
        "results_lr = cross_validate(model_lr, train_data['clean_posts'], train_data['type'], cv=kfolds,\n",
        "                          scoring=scoring, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsTIayh1s2oT"
      },
      "outputs": [],
      "source": [
        "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_acc']),\n",
        "                                                  np.std(results_lr['test_acc'])))\n",
        "\n",
        "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_f1_micro']),\n",
        "                                            np.std(results_lr['test_f1_micro'])))\n",
        "\n",
        "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_lr['test_neg_log_loss']),\n",
        "                                                 np.std(-1*results_lr['test_neg_log_loss'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNwtvPn0tXAW"
      },
      "source": [
        "# **Visualization**\n",
        "As our Last model `(Logistic Regression)` gives high accuracy so we will apply our last model to whole users comments.\n",
        "\n",
        "Let's see what is the most common user personalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cdLwBwQt9Is"
      },
      "outputs": [],
      "source": [
        "model_lr.fit(train_data['clean_posts'], train_data['type'])\n",
        "pred_all = model_lr.predict(forum_data_agg['clean_messages'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tato0ax3zFF2"
      },
      "outputs": [],
      "source": [
        "cnt_all = np.unique(pred_all, return_counts=True)\n",
        "\n",
        "pred_df = pd.DataFrame({'personality': cnt_all[0], 'count': cnt_all[1]},\n",
        "                      columns=['personality', 'count'], index=None)\n",
        "\n",
        "pred_df.sort_values('count', ascending=False, inplace=True)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(x=pred_df['personality'], y=pred_df['count'], alpha=0.8)\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Personality', fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qi2PxGszTtp"
      },
      "outputs": [],
      "source": [
        "pred_df['percent'] = pred_df['count']/pred_df['count'].sum()\n",
        "pred_df['description'] = pred_df['personality'].apply(lambda x: ' '.join([mbti[l] for l in list(x)]))\n",
        "pred_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pFEcPibzY0I"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "import plotly.offline as py\n",
        "labels = pred_df['description']\n",
        "sizes = pred_df['percent']*100\n",
        "\n",
        "trace = go.Pie(labels=labels, values=sizes)\n",
        "layout = go.Layout(title='Kaggle Personality Distribution')\n",
        "\n",
        "data = [trace]\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "py.iplot(fig)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}